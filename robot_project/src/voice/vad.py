"""
è¯­éŸ³æ´»åŠ¨æ£€æµ‹æ¨¡å—
ä½¿ç”¨ webrtcvad å®ç°å®æ—¶è¯­éŸ³æ£€æµ‹ï¼ˆæŒç»­ç›‘å¬æ ¸å¿ƒï¼‰
"""

import webrtcvad
import collections
from typing import Iterator, Optional


class VADDetector:
    """è¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨"""

    def __init__(self, aggressiveness: int = 3, sample_rate: int = 16000):
        """
        åˆå§‹åŒ–VADæ£€æµ‹å™¨

        Args:
            aggressiveness: æ£€æµ‹ä¸¥æ ¼ç¨‹åº¦ (0-3)
                           0: æœ€å®½æ¾ï¼ˆå®¹æ˜“è§¦å‘ï¼‰
                           1: ä¸€èˆ¬å®½æ¾
                           2: ä¸€èˆ¬ä¸¥æ ¼
                           3: æœ€ä¸¥æ ¼ï¼ˆæ¨èï¼Œå‡å°‘è¯¯è§¦å‘ï¼‰
            sample_rate: é‡‡æ ·ç‡ï¼ˆå¿…é¡»æ˜¯ 8000, 16000, 32000, 48000 ä¹‹ä¸€ï¼‰
        """
        self.vad = webrtcvad.Vad(aggressiveness)
        self.sample_rate = sample_rate

        # éªŒè¯é‡‡æ ·ç‡
        if sample_rate not in [8000, 16000, 32000, 48000]:
            raise ValueError(f"é‡‡æ ·ç‡å¿…é¡»æ˜¯ 8000, 16000, 32000, 48000 ä¹‹ä¸€ï¼Œå½“å‰: {sample_rate}")

        # VAD åªèƒ½å¤„ç† 10ms, 20ms, 30ms çš„å¸§
        # å¯¹äº 16kHzï¼Œ10ms = 160 samples = 320 bytes (16bit)
        self.frame_duration_ms = 30  # ä½¿ç”¨ 30ms å¸§
        self.frame_size = int(sample_rate * self.frame_duration_ms / 1000) * 2  # bytes

        # è¯­éŸ³æ£€æµ‹å‚æ•°
        self.padding_duration_ms = 300  # é™éŸ³å¡«å……æ—¶é•¿ï¼ˆæ£€æµ‹åˆ°é™éŸ³åç»§ç»­å½•éŸ³çš„æ—¶é—´ï¼‰
        self.speech_start_frames = 10   # è¿ç»­å¤šå°‘å¸§æ£€æµ‹åˆ°è¯­éŸ³æ‰å¼€å§‹å½•éŸ³
        self.speech_end_frames = 20     # è¿ç»­å¤šå°‘å¸§é™éŸ³æ‰ç»“æŸå½•éŸ³

        print(f"ğŸ”Š VADæ£€æµ‹å™¨åˆå§‹åŒ–:")
        print(f"   ä¸¥æ ¼åº¦: {aggressiveness}/3")
        print(f"   é‡‡æ ·ç‡: {sample_rate} Hz")
        print(f"   å¸§å¤§å°: {self.frame_size} bytes ({self.frame_duration_ms}ms)")

    def is_speech(self, audio_chunk: bytes) -> bool:
        """
        åˆ¤æ–­éŸ³é¢‘å—æ˜¯å¦åŒ…å«è¯­éŸ³

        Args:
            audio_chunk: éŸ³é¢‘æ•°æ®ï¼ˆå¿…é¡»æ˜¯ 10/20/30ms çš„å¸§ï¼‰

        Returns:
            True if åŒ…å«è¯­éŸ³, False otherwise
        """
        # ç¡®ä¿éŸ³é¢‘å—å¤§å°æ­£ç¡®
        if len(audio_chunk) != self.frame_size:
            # å¡«å……æˆ–æˆªæ–­åˆ°æ­£ç¡®å¤§å°
            if len(audio_chunk) < self.frame_size:
                audio_chunk += b'\x00' * (self.frame_size - len(audio_chunk))
            else:
                audio_chunk = audio_chunk[:self.frame_size]

        try:
            return self.vad.is_speech(audio_chunk, self.sample_rate)
        except:
            return False

    def detect_speech_segments(self, audio_stream: Iterator[bytes]) -> Optional[bytes]:
        """
        ä»éŸ³é¢‘æµä¸­æ£€æµ‹è¯­éŸ³ç‰‡æ®µï¼ˆæŒç»­ç›‘å¬æ ¸å¿ƒï¼‰

        å·¥ä½œåŸç†ï¼š
        1. æŒç»­ç›‘å¬éŸ³é¢‘æµ
        2. æ£€æµ‹åˆ°è¿ç»­çš„è¯­éŸ³å¸§æ—¶å¼€å§‹å½•éŸ³
        3. æ£€æµ‹åˆ°è¿ç»­çš„é™éŸ³å¸§æ—¶ç»“æŸå½•éŸ³å¹¶è¿”å›

        Args:
            audio_stream: éŸ³é¢‘æµè¿­ä»£å™¨ï¼ˆæ¥è‡ª AudioDevice.start_stream()ï¼‰

        Returns:
            æ£€æµ‹åˆ°çš„è¯­éŸ³ç‰‡æ®µï¼ˆPCMæ•°æ®ï¼‰ï¼Œå¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°è¿”å› None
        """
        # ä½¿ç”¨ç¯å½¢ç¼“å†²åŒº
        ring_buffer = collections.deque(maxlen=self.speech_end_frames)
        triggered = False  # æ˜¯å¦å·²è§¦å‘å½•éŸ³
        voiced_frames = []  # å½•éŸ³ç¼“å†²

        speech_count = 0  # è¿ç»­è¯­éŸ³å¸§è®¡æ•°
        silence_count = 0  # è¿ç»­é™éŸ³å¸§è®¡æ•°

        for audio_chunk in audio_stream:
            # åˆ†å‰²ä¸ºå¤šä¸ªVADå¸§
            num_frames = len(audio_chunk) // self.frame_size

            for i in range(num_frames):
                frame = audio_chunk[i * self.frame_size:(i + 1) * self.frame_size]

                if len(frame) < self.frame_size:
                    continue

                is_speech = self.is_speech(frame)

                if not triggered:
                    # ç­‰å¾…è§¦å‘çŠ¶æ€
                    ring_buffer.append((frame, is_speech))

                    if is_speech:
                        speech_count += 1
                        silence_count = 0
                    else:
                        speech_count = 0
                        silence_count += 1

                    # æ£€æµ‹åˆ°è¶³å¤Ÿçš„è¯­éŸ³å¸§ï¼Œå¼€å§‹å½•éŸ³
                    if speech_count >= self.speech_start_frames:
                        triggered = True
                        print("ğŸ¤ æ£€æµ‹åˆ°è¯´è¯ï¼Œå¼€å§‹å½•éŸ³...")

                        # å°†ç¯å½¢ç¼“å†²åŒºä¸­çš„æ•°æ®åŠ å…¥å½•éŸ³
                        for f, s in ring_buffer:
                            voiced_frames.append(f)

                        ring_buffer.clear()
                        speech_count = 0
                        silence_count = 0

                else:
                    # å½•éŸ³çŠ¶æ€
                    voiced_frames.append(frame)
                    ring_buffer.append((frame, is_speech))

                    if not is_speech:
                        silence_count += 1
                        speech_count = 0
                    else:
                        silence_count = 0
                        speech_count += 1

                    # æ£€æµ‹åˆ°è¶³å¤Ÿçš„é™éŸ³å¸§ï¼Œç»“æŸå½•éŸ³
                    if silence_count >= self.speech_end_frames:
                        print("ğŸ”‡ æ£€æµ‹åˆ°é™éŸ³ï¼Œå½•éŸ³ç»“æŸ")

                        # è¿”å›å½•éŸ³æ•°æ®ï¼ˆå»æ‰æœ«å°¾çš„é™éŸ³ï¼‰
                        voiced_audio = b''.join(voiced_frames[:-self.speech_end_frames])
                        return voiced_audio if voiced_audio else None

        # éŸ³é¢‘æµç»“æŸ
        if triggered and voiced_frames:
            return b''.join(voiced_frames)

        return None

    def filter_silence(self, audio_data: bytes) -> list:
        """
        è¿‡æ»¤éŸ³é¢‘ä¸­çš„é™éŸ³éƒ¨åˆ†

        Args:
            audio_data: å®Œæ•´éŸ³é¢‘æ•°æ®

        Returns:
            åŒ…å«è¯­éŸ³çš„éŸ³é¢‘ç‰‡æ®µåˆ—è¡¨
        """
        segments = []
        current_segment = []
        is_speaking = False

        # åˆ†å‰²ä¸ºVADå¸§
        for i in range(0, len(audio_data), self.frame_size):
            frame = audio_data[i:i + self.frame_size]

            if len(frame) < self.frame_size:
                break

            if self.is_speech(frame):
                current_segment.append(frame)
                is_speaking = True
            else:
                if is_speaking and current_segment:
                    # ç»“æŸä¸€ä¸ªè¯­éŸ³ç‰‡æ®µ
                    segments.append(b''.join(current_segment))
                    current_segment = []
                    is_speaking = False

        # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_segment:
            segments.append(b''.join(current_segment))

        return segments
