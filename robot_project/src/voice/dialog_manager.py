"""
åŒå‘è¯­éŸ³å¯¹è¯ç®¡ç†å™¨
å®ç°æŒç»­ç›‘å¬ + å®æ—¶æµå¼è¯†åˆ« + å£°çº¹è¯†åˆ«

æ ¸å¿ƒç‰¹æ€§ï¼š
1. è¾¹è¯´è¾¹è¯†åˆ« - å®æ—¶æ˜¾ç¤ºè¯†åˆ«ç»“æœ
2. ä½å»¶è¿Ÿ - è¾¹åˆæˆè¾¹æ’­æ”¾
3. å£°çº¹è¯†åˆ« - è‡ªåŠ¨è¯†åˆ«è¯´è¯äººï¼Œé™Œç”Ÿäººä¸»åŠ¨è¯¢é—®
"""

import asyncio
import threading
import struct
import re
import sys
from typing import Optional
from queue import Queue, Empty

# å¯¼å…¥ ASR ç»“æœç±»å‹
from src.voice.asr import ASRResult


class VoiceDialogManager:
    """åŒå‘è¯­éŸ³å¯¹è¯ç®¡ç†å™¨"""

    def __init__(self, brain, audio_device, vad, asr, tts, speaker_id=None):
        """
        åˆå§‹åŒ–å¯¹è¯ç®¡ç†å™¨

        Args:
            brain: AIå¤§è„‘å®ä¾‹
            audio_device: éŸ³é¢‘è®¾å¤‡å®ä¾‹
            vad: VADæ£€æµ‹å™¨å®ä¾‹
            asr: ASRè¯†åˆ«å™¨å®ä¾‹
            tts: TTSåˆæˆå™¨å®ä¾‹
            speaker_id: å£°çº¹è¯†åˆ«å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.brain = brain
        self.audio_device = audio_device
        self.vad = vad
        self.asr = asr
        self.tts = tts
        self.speaker_id = speaker_id

        # çŠ¶æ€æ§åˆ¶
        self.is_speaking = False  # TTSæ˜¯å¦åœ¨æ’­æ”¾
        self.is_listening = False  # æ˜¯å¦æ­£åœ¨è¯†åˆ«ç”¨æˆ·è¯­éŸ³
        self.running = True  # æ˜¯å¦ç»§ç»­è¿è¡Œ
        self.wait_after_speak = 0  # æ’­æ”¾ç»“æŸåç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        self._aec_enabled = bool(getattr(self.audio_device, "aec", None) and self.audio_device.aec.enabled)

        # å£°çº¹è¯†åˆ«çŠ¶æ€
        self.current_speaker_name = None  # å½“å‰è¯´è¯äººåå­—
        self.awaiting_name = False  # æ˜¯å¦åœ¨ç­‰å¾…ç”¨æˆ·å‘ŠçŸ¥åå­—

        # å®æ—¶è¯†åˆ«ç›¸å…³
        self.audio_queue = None  # éŸ³é¢‘æ•°æ®é˜Ÿåˆ—ï¼ˆç”¨äºå®æ—¶ ASRï¼‰
        self.stop_asr_event = None  # ASR åœæ­¢äº‹ä»¶
        self.current_text = ""  # å½“å‰è¯†åˆ«çš„æ–‡æœ¬
        self.last_displayed_text = ""  # ä¸Šæ¬¡æ˜¾ç¤ºçš„æ–‡æœ¬

        # éŸ³é¢‘ç¼“å­˜ï¼ˆç”¨äºå£°çº¹è¯†åˆ«ï¼‰
        self.audio_buffer = []

        # éŸ³é¢‘æµ
        self.audio_stream = None

        # é€€å‡ºå…³é”®è¯
        self.exit_keywords = ['é€€å‡º', 'å†è§', 'æ‹œæ‹œ', 'ç»“æŸ']

    async def run(self):
        """ä¸»è¿è¡Œå…¥å£"""
        print("=" * 50)
        print("  å°å¯çˆ± - åŒå‘è¯­éŸ³å¯¹è¯æ¨¡å¼ï¼ˆå®æ—¶è¯†åˆ«ï¼‰")
        print("=" * 50)
        print()

        # è‡ªæˆ‘ä»‹ç»
        intro = self.brain.introduce()
        print(f"ğŸ¤– {self.brain.persona.persona['name']}: {intro}")
        print()

        # æ’­æ”¾è‡ªæˆ‘ä»‹ç»
        await self._speak(intro, interruptible=False)

        print("=" * 50)
        print("  è¾¹è¯´è¾¹è¯†åˆ«æ¨¡å¼å·²å¯åŠ¨")
        print("  è¯´\"é€€å‡º\"æˆ–\"å†è§\"ç»“æŸå¯¹è¯")
        print("=" * 50)
        print()

        # å¯åŠ¨éŸ³é¢‘æµ
        self.audio_stream = self.audio_device.start_stream()

        # ä¸»å¾ªç¯
        try:
            await self._main_loop()
        except KeyboardInterrupt:
            print("\n\næ­£åœ¨é€€å‡º...")
        finally:
            self.running = False
            self.audio_device.stop_stream()
            self.brain.memory.save_remaining()
            print("ğŸ’¾ è®°å¿†å·²ä¿å­˜")
            print("ğŸ‘‹ å†è§ï¼")

    async def _main_loop(self):
        """ä¸»å¾ªç¯ï¼šæ£€æµ‹è¯´è¯ â†’ å®æ—¶è¯†åˆ« â†’ å¤„ç†å¯¹è¯"""
        print("[DEBUG] è¿›å…¥ä¸»å¾ªç¯")
        while self.running:
            try:
                # æ²¡æœ‰ AEC æ—¶ï¼Œç”¨â€œæš‚åœç›‘å¬ + ç­‰å¾…æ¶ˆæ•£â€è§„é¿å›å£°ï¼›
                # å¯ç”¨ AEC(ec) åï¼Œå…è®¸å¤–æ”¾åŒæ—¶ç›‘å¬ï¼ˆå¹¶åœ¨æ’­æŠ¥é˜¶æ®µåšæ’è¯æ£€æµ‹ï¼‰ã€‚
                if not self._aec_enabled:
                    # TTS æ’­æ”¾æ—¶æš‚åœ
                    if self.is_speaking:
                        await asyncio.sleep(0.1)
                        continue

                    # æ’­æ”¾ç»“æŸåç­‰å¾…å›å£°æ¶ˆæ•£
                    if self.wait_after_speak > 0:
                        await asyncio.sleep(0.1)
                        self.wait_after_speak -= 0.1
                        continue

                # ç­‰å¾…ç”¨æˆ·å¼€å§‹è¯´è¯ï¼ˆä½¿ç”¨ VAD æ£€æµ‹ï¼‰
                print("ğŸ¤ ç­‰å¾…è¯´è¯...", flush=True)
                speech_started, pre_buffer = await self._wait_for_speech_start()

                if not speech_started or not self.running:
                    continue

                # æ²¡æœ‰ AEC æ—¶ï¼Œæ’­æ”¾æœŸé—´ä¸å¤„ç†ï¼›AEC å¯ç”¨æ—¶å…è®¸â€œè¾¹æ’­è¾¹å¬â€
                if self.is_speaking and not self._aec_enabled:
                    continue

                # å¼€å§‹å®æ—¶è¯†åˆ«ï¼ˆä¼ å…¥é¢„ç¼“å†²éŸ³é¢‘ï¼‰
                print("ğŸ”Š å¼€å§‹å®æ—¶è¯†åˆ«...")
                final_text, audio_data = await self._realtime_recognize(pre_buffer)

                if not final_text:
                    print("âš ï¸ æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹")
                    continue

                # å¤„ç†è¯†åˆ«ç»“æœ
                await self._handle_speech(final_text, audio_data)

            except Exception as e:
                print(f"âš ï¸ å¤„ç†é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()

    async def _wait_for_speech_start(self) -> tuple:
        """
        ç­‰å¾…ç”¨æˆ·å¼€å§‹è¯´è¯ï¼ˆä½¿ç”¨ VAD æ£€æµ‹ï¼‰

        Returns:
            (speech_started, pre_buffer) - æ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³ï¼Œé¢„ç¼“å†²çš„éŸ³é¢‘å¸§åˆ—è¡¨
        """
        # é¢„ç¼“å†²ï¼šä¿å­˜æœ€è¿‘çš„éŸ³é¢‘å¸§ï¼Œé˜²æ­¢ä¸¢å¤±å¼€å¤´
        pre_buffer = []
        pre_buffer_max = 15  # ä¿ç•™æœ€è¿‘ 15 å¸§ï¼ˆçº¦ 300msï¼‰

        def read_and_check():
            """åœ¨çº¿ç¨‹ä¸­è¯»å–éŸ³é¢‘å¹¶æ£€æµ‹è¯­éŸ³ï¼ˆæ”¯æŒéæ•´å¸§é•¿åº¦çš„ chunkï¼‰"""
            try:
                chunk = self.audio_stream.read()
                # AudioDevice.read() è¿”å›çš„æ˜¯ä»»æ„é•¿åº¦ PCM bytesï¼›
                # webrtcvad åªèƒ½å¤„ç† 10/20/30ms å¸§ï¼Œå› æ­¤è¿™é‡ŒæŒ‰ VAD çš„ frame_size åˆ‡åˆ†ååšèšåˆåˆ¤å®šã€‚
                frame_size = getattr(self.vad, "frame_size", None)
                if not frame_size or frame_size <= 0:
                    is_speech = self.vad.is_speech(chunk)
                else:
                    frames = [chunk[i:i + frame_size] for i in range(0, len(chunk), frame_size)]
                    speech_votes = 0
                    total_votes = 0
                    for f in frames:
                        if len(f) < frame_size:
                            continue
                        total_votes += 1
                        if self.vad.is_speech(f):
                            speech_votes += 1
                    # è¶…è¿‡ä¸€åŠå¸§åˆ¤ä¸ºè¯­éŸ³ï¼Œé™ä½è¯¯è§¦å‘/æ¼æ£€ï¼ˆå°¤å…¶æ˜¯å¤–æ”¾ç¯å¢ƒï¼‰
                    is_speech = total_votes > 0 and (speech_votes / total_votes) >= 0.5
                return chunk, is_speech
            except:
                return None, False

        loop = asyncio.get_event_loop()
        consecutive_speech = 0
        # chunk_size è°ƒæ•´åˆ° 30ms å·¦å³åï¼Œ3 å¸§çº¦ 90msï¼›å¦‚æœä½ è§‰å¾—ä»ç„¶æ…¢ï¼Œå¯é™ä¸º 2ã€‚
        speech_threshold = 3  # è¿ç»­ N å¸§æ£€æµ‹åˆ°è¯­éŸ³æ‰è®¤ä¸ºå¼€å§‹è¯´è¯

        while self.running and not self.is_speaking:
            try:
                chunk, is_speech = await loop.run_in_executor(None, read_and_check)

                if chunk:
                    # æ·»åŠ åˆ°é¢„ç¼“å†²
                    pre_buffer.append(chunk)
                    if len(pre_buffer) > pre_buffer_max:
                        pre_buffer.pop(0)

                if is_speech:
                    consecutive_speech += 1
                    if consecutive_speech >= speech_threshold:
                        return True, pre_buffer
                else:
                    consecutive_speech = 0

                await asyncio.sleep(0.02)
            except:
                return False, []

        return False, []

    async def _realtime_recognize(self, pre_buffer: list = None) -> tuple:
        """
        å®æ—¶æµå¼è¯†åˆ«

        Args:
            pre_buffer: é¢„ç¼“å†²çš„éŸ³é¢‘å¸§åˆ—è¡¨ï¼ˆVADæ£€æµ‹æœŸé—´æ”¶é›†çš„ï¼‰

        Returns:
            (final_text, audio_data) - æœ€ç»ˆæ–‡æœ¬å’ŒéŸ³é¢‘æ•°æ®
        """
        # åˆå§‹åŒ–
        self.audio_queue = asyncio.Queue()
        self.stop_asr_event = asyncio.Event()
        self.current_text = ""
        self.last_displayed_text = ""
        self.audio_buffer = []
        self.is_listening = True

        # å…ˆæŠŠé¢„ç¼“å†²çš„éŸ³é¢‘åŠ å…¥é˜Ÿåˆ—å’Œç¼“å†²åŒº
        if pre_buffer:
            for chunk in pre_buffer:
                self.audio_buffer.append(chunk)
                await self.audio_queue.put(chunk)

        final_text = ""

        def on_result(result: ASRResult):
            """è¯†åˆ«ç»“æœå›è°ƒ"""
            nonlocal final_text
            self.current_text = result.text

            # å®æ—¶æ˜¾ç¤ºï¼ˆè¦†ç›–ä¸Šä¸€è¡Œï¼‰
            if result.text != self.last_displayed_text:
                # æ¸…é™¤å½“å‰è¡Œå¹¶æ˜¾ç¤ºæ–°æ–‡æœ¬
                display_text = result.text[:50] + "..." if len(result.text) > 50 else result.text
                prefix = "âœ…" if result.is_final else "ğŸ“"
                sys.stdout.write(f"\r{prefix} {display_text}                    ")
                sys.stdout.flush()
                self.last_displayed_text = result.text

            if result.is_final:
                final_text = result.text
                print()  # æ¢è¡Œ

        # å¯åŠ¨éŸ³é¢‘å‘é€ä»»åŠ¡
        audio_task = asyncio.create_task(self._send_audio_to_asr())

        # å¯åŠ¨ ASR è¯†åˆ«
        try:
            result = await self.asr.recognize_realtime(
                audio_queue=self.audio_queue,
                on_result=on_result,
                stop_event=self.stop_asr_event,
                end_window_size=800  # 800ms é™éŸ³åˆ¤åœ
            )
            if result:
                final_text = result
        except Exception as e:
            print(f"\nâš ï¸ è¯†åˆ«é”™è¯¯: {e}")

        # åœæ­¢éŸ³é¢‘å‘é€
        self.stop_asr_event.set()
        audio_task.cancel()
        try:
            await audio_task
        except asyncio.CancelledError:
            pass

        self.is_listening = False

        # åˆå¹¶éŸ³é¢‘æ•°æ®ï¼ˆç”¨äºå£°çº¹è¯†åˆ«ï¼‰
        audio_data = b''.join(self.audio_buffer) if self.audio_buffer else b''

        return final_text, audio_data

    async def _send_audio_to_asr(self):
        """æŒç»­ä»éº¦å…‹é£è¯»å–éŸ³é¢‘å¹¶å‘é€ç»™ ASR"""
        loop = asyncio.get_event_loop()

        def read_audio():
            try:
                return self.audio_stream.read()
            except:
                return None

        while not self.stop_asr_event.is_set() and self.running:
            try:
                # åœ¨çº¿ç¨‹æ± ä¸­è¯»å–éŸ³é¢‘ï¼ˆé˜»å¡æ“ä½œï¼‰
                chunk = await loop.run_in_executor(None, read_audio)

                if chunk and not self.is_speaking:
                    # ä¿å­˜åˆ°ç¼“å†²åŒºï¼ˆç”¨äºå£°çº¹è¯†åˆ«ï¼‰
                    self.audio_buffer.append(chunk)
                    # å‘é€ç»™ ASR
                    await self.audio_queue.put(chunk)

                await asyncio.sleep(0.01)
            except asyncio.CancelledError:
                break
            except Exception as e:
                break

    async def _handle_speech(self, user_text: str, audio_data: bytes):
        """å¤„ç†è¯†åˆ«å®Œæˆçš„è¯­éŸ³"""
        print(f"\nğŸ“ æœ€ç»ˆç»“æœ: {user_text}")

        # å¦‚æœåœ¨ç­‰å¾…åå­—ï¼Œç›´æ¥å¤„ç†æ³¨å†Œæµç¨‹
        if self.awaiting_name:
            await self._handle_name_response(user_text, audio_data)
            return

        # å£°çº¹è¯†åˆ«
        speaker_name = await self._identify_speaker(audio_data, user_text)

        # å¦‚æœè§¦å‘äº†è¯¢é—®åå­—ï¼Œç›´æ¥è¿”å›
        if self.awaiting_name:
            return

        # æ˜¾ç¤ºè¯´è¯äºº
        if speaker_name:
            print(f"\nğŸ‘¤ {speaker_name}: {user_text}")
        else:
            print(f"\nğŸ‘¤ ä½ : {user_text}")

        # é€€å‡ºæ£€æµ‹
        if any(kw in user_text for kw in self.exit_keywords):
            farewell = self.brain.chat("å†è§", speaker=speaker_name, debug=False)
            print(f"\nğŸ¤– {self.brain.persona.persona['name']}: {farewell}")
            await self._speak(farewell, interruptible=False)
            self.running = False
            return

        # å¯¹è¯
        reply = self.brain.chat(user_text, speaker=speaker_name, debug=False)
        print(f"\nğŸ¤– {self.brain.persona.persona['name']}: {reply}")

        # æ’­æ”¾
        await self._speak(reply, interruptible=True)

        print("\n" + "-" * 30)

    async def _identify_speaker(self, audio_data: bytes, user_text: str) -> Optional[str]:
        """è¯†åˆ«è¯´è¯äºº"""
        if self.speaker_id is None or not audio_data:
            return self.current_speaker_name

        # å£°çº¹è¯†åˆ«
        speaker_id_result, similarity, embedding = self.speaker_id.identify(audio_data)

        if speaker_id_result:
            name = self.speaker_id.get_speaker_name(speaker_id_result)
            self.current_speaker_name = name
            print(f"ğŸ¯ å£°çº¹è¯†åˆ«: {name} (ç›¸ä¼¼åº¦: {similarity:.2f})")

            if embedding is not None:
                self.speaker_id.update_embedding(speaker_id_result, embedding)

            return name
        else:
            if embedding is not None:
                print(f"â“ æœªè¯†åˆ«çš„å£°çº¹ (æœ€é«˜ç›¸ä¼¼åº¦: {similarity:.2f})")
                self.speaker_id.set_pending_registration(embedding)
                await self._ask_for_name()
                return None

        return self.current_speaker_name

    async def _ask_for_name(self):
        """è¯¢é—®é™Œç”Ÿäººçš„åå­—"""
        self.awaiting_name = True
        ask_text = "ä½ å¥½å‘€~æˆ‘å¥½åƒè¿˜ä¸è®¤è¯†ä½ å‘¢ï¼Œä½ å«ä»€ä¹ˆåå­—å‘€ï¼Ÿ"
        print(f"\nğŸ¤– {self.brain.persona.persona['name']}: {ask_text}")
        await self._speak(ask_text, interruptible=False)

    async def _handle_name_response(self, user_text: str, audio_data: bytes):
        """å¤„ç†ç”¨æˆ·å‘ŠçŸ¥åå­—çš„å›å¤"""
        print(f"\nğŸ‘¤ ä½ : {user_text}")

        # å…ˆå°è¯•æ­£åˆ™æå–
        name = self._extract_name(user_text)

        # å¦‚æœæ­£åˆ™å¤±è´¥ï¼Œç”¨AIç†è§£æ„å›¾
        if not name:
            result = await self._ai_understand_name(user_text)
            if result.get('is_name'):
                name = result.get('name')
            elif result.get('skip'):
                # ç”¨æˆ·ä¸æƒ³è¯´åå­—ï¼Œè·³è¿‡æ³¨å†Œ
                print(f"ğŸ“ ç”¨æˆ·è·³è¿‡æ³¨å†Œ")
                self.awaiting_name = False
                if self.speaker_id:
                    self.speaker_id.cancel_registration()
                reply = result.get('reply', "å¥½çš„ï¼Œé‚£æˆ‘ä»¬å…ˆèŠåˆ«çš„å§~")
                print(f"\nğŸ¤– {self.brain.persona.persona['name']}: {reply}")
                await self._speak(reply, interruptible=False)
                return
            elif result.get('other_intent'):
                # ç”¨æˆ·åœ¨è¯´åˆ«çš„äº‹æƒ…ï¼Œå…ˆå›åº”å†ç»§ç»­é—®åå­—
                print(f"ğŸ“ ç”¨æˆ·åœ¨è¯´å…¶ä»–äº‹æƒ…")
                reply = result.get('reply', "")
                if reply:
                    print(f"\nğŸ¤– {self.brain.persona.persona['name']}: {reply}")
                    await self._speak(reply, interruptible=False)
                # ç»§ç»­é—®åå­—
                ask_text = "å¯¹äº†ï¼Œä½ è¿˜æ²¡å‘Šè¯‰æˆ‘ä½ å«ä»€ä¹ˆåå­—å‘¢~"
                print(f"\nğŸ¤– {self.brain.persona.persona['name']}: {ask_text}")
                await self._speak(ask_text, interruptible=False)
                return

        print(f"ğŸ“ æå–åå­—: {name if name else 'æœªè¯†åˆ«åˆ°'}")

        if name:
            if self.speaker_id and self.speaker_id.has_pending_registration():
                self.speaker_id.complete_registration(name)
                self.current_speaker_name = name
                self.awaiting_name = False

                welcome = f"åŸæ¥æ˜¯{name}å‘€ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ~æˆ‘æ˜¯å°å¯çˆ±ï¼Œä»¥åæˆ‘å°±èƒ½è®¤å‡ºä½ çš„å£°éŸ³å•¦ï¼"
                print(f"\nğŸ¤– {self.brain.persona.persona['name']}: {welcome}")
                await self._speak(welcome, interruptible=False)

                self.brain.memory.add_fact(f"è®¤è¯†äº†æ–°æœ‹å‹{name}ï¼Œå·²è®°ä½taçš„å£°çº¹")
            else:
                self.awaiting_name = False
        else:
            retry_text = "æŠ±æ­‰ï¼Œæˆ‘æ²¡å¬æ¸…æ¥šä½ çš„åå­—ï¼Œèƒ½å†è¯´ä¸€æ¬¡å—ï¼Ÿä½ ä¹Ÿå¯ä»¥è¯´'ç®—äº†'è·³è¿‡~"
            print(f"\nğŸ¤– {self.brain.persona.persona['name']}: {retry_text}")
            await self._speak(retry_text, interruptible=False)

    async def _ai_understand_name(self, user_text: str) -> dict:
        """ç”¨AIç†è§£ç”¨æˆ·æ˜¯å¦åœ¨è¯´åå­—"""
        prompt = f"""ç”¨æˆ·åˆšæ‰è¢«é—®"ä½ å«ä»€ä¹ˆåå­—"ï¼Œå›ç­”äº†ï¼š"{user_text}"

è¯·åˆ¤æ–­ï¼š
1. ç”¨æˆ·æ˜¯å¦åœ¨å‘Šè¯‰è‡ªå·±çš„åå­—ï¼Ÿ
2. å¦‚æœæ˜¯ï¼Œåå­—æ˜¯ä»€ä¹ˆï¼Ÿ
3. å¦‚æœä¸æ˜¯ï¼Œç”¨æˆ·æ˜¯æƒ³è·³è¿‡ï¼ˆå¦‚"ç®—äº†""ä¸è¯´äº†"ï¼‰ï¼Œè¿˜æ˜¯åœ¨è¯´å…¶ä»–äº‹æƒ…ï¼Ÿ

è¯·ç”¨JSONæ ¼å¼å›ç­”ï¼ˆä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼‰ï¼š
{{"is_name": true/false, "name": "åå­—æˆ–null", "skip": true/false, "other_intent": true/false, "reply": "å¦‚æœç”¨æˆ·åœ¨è¯´å…¶ä»–äº‹æƒ…ï¼Œç®€çŸ­å›åº”"}}"""

        try:
            # è°ƒç”¨AIï¼ˆä½¿ç”¨è¾ƒçŸ­çš„max_tokensåŠ å¿«å“åº”ï¼‰
            from openai import OpenAI
            import json

            # å¤ç”¨brainçš„å®¢æˆ·ç«¯é…ç½®
            if hasattr(self.brain, 'client') and self.brain.client:
                response = self.brain.client.chat.completions.create(
                    model=self.brain.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,
                    max_tokens=150
                )
                result_text = response.choices[0].message.content.strip()

                # è§£æJSON
                # å¤„ç†å¯èƒ½çš„markdownä»£ç å—
                if result_text.startswith('```'):
                    result_text = result_text.split('```')[1]
                    if result_text.startswith('json'):
                        result_text = result_text[4:]
                    result_text = result_text.strip()

                return json.loads(result_text)
        except Exception as e:
            print(f"âš ï¸ AIç†è§£å¤±è´¥: {e}")

        return {"is_name": False, "name": None, "skip": False, "other_intent": False}

    def _extract_name(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå–åå­—"""
        text = text.strip()
        text = re.sub(r'(æˆ‘æ˜¯|æˆ‘å«){2,}', r'\1', text)

        patterns = [
            r"æˆ‘(?:æ˜¯|å«|çš„åå­—æ˜¯|åå«)[\s]*([^\s,ï¼Œã€‚ï¼!ï¼Ÿ?æˆ‘æ˜¯å«]{2,4})",
            r"å«æˆ‘[\s]*([^\s,ï¼Œã€‚ï¼!ï¼Ÿ?]{2,4})",
            r"^([^\s,ï¼Œã€‚ï¼!ï¼Ÿ?æˆ‘æ˜¯å«]{2,4})$",
        ]

        # æ— æ•ˆè¯åˆ—è¡¨
        skip_words = [
            'ä»€ä¹ˆ', 'è°', 'ä½ å¥½', 'å—¯', 'å•Š', 'å“¦', 'å‘ƒ', 'é‚£ä¸ª', 'è¿™ä¸ª',
            'å¹²å˜›', 'æ€ä¹ˆ', 'å¥½çš„', 'çŸ¥é“', 'å¯ä»¥', 'ä¸æ˜¯', 'æ²¡æœ‰',
            'é€€å‡º', 'å†è§', 'æ‹œæ‹œ', 'ç»“æŸ', 'åœæ­¢', 'å…³é—­'
        ]

        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                name = match.group(1).strip()
                # å»æ‰æ ‡ç‚¹åæ£€æŸ¥
                clean_name = re.sub(r'[ï¼Ÿ?ï¼!ã€‚ï¼Œ,ã€]', '', name)
                if 1 < len(clean_name) <= 4:
                    if clean_name not in skip_words and not clean_name.endswith('å—'):
                        return clean_name

        clean_text = re.sub(r'^(æˆ‘æ˜¯|æˆ‘å«|å«æˆ‘|æˆ‘çš„åå­—æ˜¯)+', '', text).strip()
        clean_text = re.sub(r'[ï¼Ÿ?ï¼!ã€‚ï¼Œ,ã€]', '', clean_text)
        if 2 <= len(clean_text) <= 4:
            if clean_text not in skip_words and not clean_text.endswith('å—'):
                return clean_text

        return None

    async def _speak(self, text: str, interruptible: bool = True):
        """è¯­éŸ³æ’­æ”¾"""
        self.is_speaking = True

        try:
            print(f"ğŸ—£ï¸ æ’­æ”¾: {text[:30]}...")

            buffer = b''
            buffer_threshold = 3200

            async for chunk in self.tts.synthesize_stream(text):
                buffer += chunk

                if len(buffer) >= buffer_threshold:
                    self.audio_device.play_audio(buffer)
                    buffer = b''

                    # AEC å¯ç”¨æ—¶å°è¯•â€œå¯æ’è¯â€ï¼šæ’­æŠ¥æœŸé—´å¿«é€Ÿåš VAD æ£€æµ‹
                    if interruptible and self._aec_enabled and self.audio_stream:
                        if await self._barge_in_check():
                            print("\nâš¡ æ£€æµ‹åˆ°æ’è¯ï¼Œåœæ­¢æ’­æŠ¥")
                            break

            if buffer:
                self.audio_device.play_audio(buffer)

            print("âœ… æ’­æ”¾å®Œæˆ")

        except Exception as e:
            print(f"âš ï¸ TTSæ’­æ”¾å¤±è´¥: {e}")

        finally:
            self.is_speaking = False
            # æ—  AEC æ—¶ç­‰å¾…å›å£°æ¶ˆæ•£ï¼›å¯ç”¨ AEC åˆ™ä¸å†ä¾èµ–ç­‰å¾…ç­–ç•¥
            if self._aec_enabled:
                self.wait_after_speak = 0
            else:
                self.wait_after_speak = 0.5

    async def _barge_in_check(self) -> bool:
        """
        æ’­æ”¾æœŸé—´å¿«é€Ÿæ£€æµ‹æ˜¯å¦æœ‰äººæ’è¯ã€‚

        è¯´æ˜ï¼š
        - AEC å¯ç”¨æ—¶ï¼ŒAudioStream è¯»å–çš„æ˜¯ /tmp/ec.outputï¼ˆå·²æ¶ˆå›å£°ï¼‰ï¼Œæ›´é€‚åˆåšæ’è¯æ£€æµ‹ã€‚
        - è¿™é‡Œåªåšè½»é‡æŠ•ç¥¨ï¼šè¯»å–å°‘é‡å¸§ï¼Œè¶…è¿‡é˜ˆå€¼è®¤ä¸ºåœ¨è¯´è¯ã€‚
        """
        if not self.audio_stream:
            return False

        loop = asyncio.get_event_loop()

        def read_chunk():
            try:
                return self.audio_stream.read()
            except Exception:
                return b""

        votes = 0
        total = 0

        # è¯»å–ä¸¤æ¬¡ï¼Œé€šå¸¸çº¦ 2 * chunk_sizeï¼ˆchunk_size é»˜è®¤ 30msï¼‰
        for _ in range(2):
            chunk = await loop.run_in_executor(None, read_chunk)
            if not chunk:
                continue

            frame_size = getattr(self.vad, "frame_size", None)
            if not frame_size:
                total += 1
                if self.vad.is_speech(chunk):
                    votes += 1
                continue

            for i in range(0, len(chunk), frame_size):
                frame = chunk[i:i + frame_size]
                if len(frame) < frame_size:
                    continue
                total += 1
                if self.vad.is_speech(frame):
                    votes += 1

        if total == 0:
            return False

        return (votes / total) >= 0.6
